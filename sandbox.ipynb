{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "from select_disease_cohort import *\n",
    "from select_patient_info import *\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "\n",
    "# Constants\n",
    "GROUP_COL = 'subject_id' # Identifier for patients\n",
    "VISIT_COL = 'hadm_id' # Identifier for visits\n",
    "ADMIT_COL = 'admittime' # Admission time\n",
    "DISCH_COL = 'dischtime' # Discharge time\n",
    "GAP = datetime.timedelta(days=14) # Gap for within readmission must occur\n",
    "DISEASE_LABEL = 'I50' # or I50 = Heart Failure, # I25 = Coronary Artery Disease, # N18 = Chronic Kidney Disease, # J44 = Chronic obstructive pulmonary disease\n",
    "VERSION = '1.0' # MIMIC-IV version 1.0 or 2.0\n",
    "\n",
    "# Results so far for READMISSION PREDICTION\n",
    "# I25, GAP=14, MIMIC-IV 2.0, --> ROC-AUC = .57\n",
    "# I25, GAP=7, MIMIC-IV 2.0, --> ROC-AUC = .52\n",
    "# I550, GAP=7, MIMIC-IV 2.0, --> ROC-AUC = .46\n",
    "# I550, GAP=30, MIMIC-IV 2.0, --> ROC-AUC = .56\n",
    "\n",
    "# Results so far for DEATH PREDICTION\n",
    "# I50, GAP=14, MIMIC-IV 2.0, --> ROC-AUC = .80\n",
    "# I25, GAP=14, MIMIC-IV 2.0, --> ROC-AUC = .66\n",
    "\n",
    "# Paths\n",
    "PATH_ADMISSIONS: str = os.path.join(\"mimiciv\", VERSION, \"core\", \"admissions.csv.gz\")\n",
    "PATH_DIAGNOSES_ICD: str = os.path.join(\"mimiciv\", VERSION, \"hosp\", \"diagnoses_icd.csv.gz\")\n",
    "PATH_PATIENTS: str = os.path.join(\"mimiciv\", VERSION, \"core\", \"patients.csv.gz\")\n",
    "PATH_OMR: str = os.path.join(\"mimiciv\", VERSION, \"hosp\", \"omr.csv.gz\")\n",
    "PATH_PRESCRIPTIONS: str = os.path.join(\"mimiciv\", VERSION, \"hosp\", \"prescriptions.csv.gz\")\n",
    "PATH_ICD_MAP: str = os.path.join(\"utils\", \"ICD9_to_ICD10_mapping.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading admissions, selecting disease cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From admissions df, standardize ICD codes (9 and 10 are mixed) and select disease cohort based on disease label (ICD10)\n",
    "visit_df = standardize_codes_and_select_cohort(disease_label=DISEASE_LABEL, path_admissions_df=PATH_ADMISSIONS, filter_deaths=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding patient df info (and basic vars from omr in case of MIMIC-IV 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appends data from patients_df to visit_df, e.g. age, gender, ethnicity, marital status\n",
    "# And also comorbidities\n",
    "visit_pts_df = get_patient_df_info(version=VERSION, visit_df=visit_df)\n",
    "visit_pts_df = get_comorbidities(get_diagnosis_icd_df(), visit_pts_df)\n",
    "visit_pts_df = visit_pts_df.sort_values(['subject_id', 'admittime']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding omr basic physical variables via row-wise operations\n",
    "visit_pts_df['bmi'] = visit_pts_df.apply(find_closest_bmi, axis=1)\n",
    "visit_pts_df.dropna(subset=['bmi'],inplace=True) # Has a lot of missing values, because omr = outpatient measurements \n",
    "\n",
    "# Create cols for systolic and diastolic blood pressure\n",
    "visit_pts_df['bp_systolic'] = visit_pts_df.apply(find_closest_bp_systolic, axis=1)\n",
    "visit_pts_df['bp_diastolic'] = visit_pts_df.apply(find_closest_bp_diastolic, axis=1)\n",
    "\n",
    "visit_pts_df.dropna(subset=['bp_systolic'],inplace=True) # Has a lot of missing values, because omr = outpatient measurements \n",
    "visit_pts_df.dropna(subset=['bp_diastolic'],inplace=True) \n",
    "\n",
    "# Adding number of medications per visit. Has no missings as it is lookup over hadm_id\n",
    "visit_pts_df['presc_meds'] = visit_pts_df.apply(get_med_prescs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Death prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task spec\n",
    "task = \"death_pred\"\n",
    "# Creating targets and balancing set)\n",
    "visit_pts_df.loc[visit_pts_df['hospital_expire_flag'] == 1, 'y_label'] = 1\n",
    "visit_pts_df.loc[visit_pts_df['dod'].isna(), 'y_label'] = 0\n",
    "visit_pts_df.loc[visit_pts_df['hospital_expire_flag'] == 0, 'y_label'] = 0\n",
    "\n",
    "# Creating targets and balancing set)\n",
    "case_df = visit_pts_df[visit_pts_df.y_label == 1]\n",
    "control_df = visit_pts_df[visit_pts_df.y_label == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df.to_csv(os.path.join('output',f\"{task + '_' + DISEASE_LABEL + '_GAP'+ str(GAP.days) + '_MIMIC_IV' + '_' + VERSION + '_case_df2.csv'}\"),index_label=False)\n",
    "control_df.to_csv(os.path.join('output', f\"{task + '_' + DISEASE_LABEL + '_GAP'+ str(GAP.days) + '_MIMIC_IV' + '_' + VERSION + '_control_df2.csv'}\"),index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "case_df = pd.read_csv(os.path.join('output',f\"{task + '_' + DISEASE_LABEL + '_GAP'+ str(GAP.days) + '_MIMIC_IV' + '_' + VERSION + '_case_df2.csv'}\"),index_col=0)\n",
    "control_df = pd.read_csv(os.path.join('output',f\"{task + '_' + DISEASE_LABEL + '_GAP'+ str(GAP.days) + '_MIMIC_IV' + '_' + VERSION + '_control_df2.csv'}\"),index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating targets and balancing set)\n",
    "visit_pts_df.loc[visit_pts_df['hospital_expire_flag'] == 1, 'y_label'] = 1\n",
    "visit_pts_df.loc[visit_pts_df['dod'].isna(), 'y_label'] = 0\n",
    "visit_pts_df.loc[visit_pts_df['hospital_expire_flag'] == 0, 'y_label'] = 0\n",
    "\n",
    "# Creating targets and balancing set)\n",
    "case_df = visit_pts_df[visit_pts_df.y_label == 1]\n",
    "control_df = visit_pts_df[visit_pts_df.y_label == 0]\n",
    "\n",
    "print(f\"Positive cases: {len(case_df)}\")\n",
    "print(f\"Negative cases: {len(control_df)}\")\n",
    "\n",
    "control_df = control_df.sample(len(case_df),random_state=42)\n",
    "\n",
    "# Concatenate\n",
    "ml_df = pd.concat([case_df, control_df], axis=0)\n",
    "ml_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Coerce variables to ints\n",
    "ml_df['los_hours'] = ml_df['los_hours'].astype(int)\n",
    "ml_df['age'] = ml_df['age'].astype(int)\n",
    "ml_df['y_label'] = ml_df['y_label'].astype(int)\n",
    "ml_df['comorbs'] = ml_df['comorbs'].astype(int)\n",
    "ml_df['bmi'] = ml_df['bmi'].astype(float)\n",
    "ml_df['bp_systolic'] = ml_df['bp_systolic'].astype(int)\n",
    "ml_df['bp_diastolic'] = ml_df['bp_diastolic'].astype(int)\n",
    "ml_df['presc_meds'] = ml_df['presc_meds'].astype(int)\n",
    "\n",
    "# Create one-hot encodings for categorical features\n",
    "categorical_columns = ['admission_type', 'admission_location',\n",
    "       'insurance', 'ethnicity', 'marital_status', 'gender']  \n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_features = encoder.fit_transform(ml_df[categorical_columns])\n",
    "feature_names = encoder.get_feature_names_out(categorical_columns)\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=feature_names)\n",
    "\n",
    "# Drop original categorical columns and concatenate with encoded ones\n",
    "ml_df = pd.concat([ml_df.drop(categorical_columns, axis=1), encoded_df], axis=1)\n",
    "\n",
    "# Selecting only properly encoded columns\n",
    "ml_df = ml_df[['los_hours',\n",
    "       'age', 'y_label', 'bmi','comorbs','bp_systolic','bp_diastolic', \n",
    "       'presc_meds',\n",
    "       'admission_type_AMBULATORY OBSERVATION', 'admission_type_DIRECT EMER.',\n",
    "       'admission_type_DIRECT OBSERVATION', 'admission_type_ELECTIVE',\n",
    "       'admission_type_EU OBSERVATION', 'admission_type_EW EMER.',\n",
    "       'admission_type_OBSERVATION ADMIT',\n",
    "       'admission_type_SURGICAL SAME DAY ADMISSION', 'admission_type_URGENT',\n",
    "       'admission_location_AMBULATORY SURGERY TRANSFER',\n",
    "       'admission_location_CLINIC REFERRAL',\n",
    "       'admission_location_EMERGENCY ROOM',\n",
    "       'admission_location_INFORMATION NOT AVAILABLE',\n",
    "       'admission_location_PACU', 'admission_location_PHYSICIAN REFERRAL',\n",
    "       'admission_location_PROCEDURE SITE',\n",
    "       'admission_location_TRANSFER FROM HOSPITAL',\n",
    "       'admission_location_TRANSFER FROM SKILLED NURSING FACILITY',\n",
    "       'admission_location_WALK-IN/SELF REFERRAL', 'insurance_Medicaid',\n",
    "       'insurance_Medicare', 'insurance_Other',\n",
    "       'ethnicity_AMERICAN INDIAN/ALASKA NATIVE', 'ethnicity_ASIAN',\n",
    "       'ethnicity_BLACK/AFRICAN AMERICAN',\n",
    "       'ethnicity_OTHER', \n",
    "       'ethnicity_UNABLE TO OBTAIN',\n",
    "       'ethnicity_UNKNOWN',\n",
    "       'ethnicity_WHITE', 'marital_status_DIVORCED', 'marital_status_MARRIED',\n",
    "       'marital_status_SINGLE', 'marital_status_WIDOWED', 'marital_status_nan',\n",
    "       'gender_F', 'gender_M']]\n",
    "\n",
    "X = ml_df.drop('y_label', axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = ml_df['y_label']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# GS\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear','saga'],\n",
    "    'tol': [1e-4, 1e-3, 1e-2], \n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7]\n",
    "    }\n",
    "\n",
    "# Create and fit grid search\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    refit=True,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\") \n",
    "\n",
    "# Training quick model\n",
    "#model = LogisticRegression(penalty='l2', max_iter=5000, C=0.1)\n",
    "#model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = grid_search.predict(X_test)\n",
    "#y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                                  estimator_name=f'LR')\n",
    "display.plot()\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write resulting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df.to_csv(os.path.join('output',f\"{task + '_' + DISEASE_LABEL + '_GAP'+ str(GAP.days) + '_MIMIC_IV' + '_' + VERSION + '_ml_df2.csv'}\"),index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readmission task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating partitions based on readmission gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task spec\n",
    "task = \"readmit_pred\"\n",
    "# These are expensive to run, so we write to 'output' folder\n",
    "case_df, control_df, invalid_df, redundant_df = partition_by_readmit(visit_pts_df,gap=GAP,group_col=GROUP_COL,visit_col=VISIT_COL,admit_col=ADMIT_COL,disch_col=DISCH_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_df.to_csv(os.path.join('output',f\"{task + '_' + DISEASE_LABEL + '_GAP'+ str(GAP.days) + '_MIMIC_IV' + '_' + VERSION + '_case_df.csv'}\"),index_label=False)\n",
    "control_df.to_csv(os.path.join('output', f\"{task + '_' + DISEASE_LABEL + '_GAP'+ str(GAP.days) + '_MIMIC_IV' + '_' + VERSION + '_control_df.csv'}\"),index_label=False)\n",
    "invalid_df.to_csv(os.path.join('output', f\"{task + '_' + DISEASE_LABEL + '_GAP'+ str(GAP.days) + '_MIMIC_IV' + '_' + VERSION + '_invalid_df.csv'}\"),index_label=False)\n",
    "redundant_df.to_csv(os.path.join('output', f\"{task + '_' + DISEASE_LABEL + '_GAP'+ str(GAP.days) + '_MIMIC_IV' + '_' + VERSION + '_redundant_df.csv'}\"),index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "case_df = pd.read_csv(os.path.join('output',f\"{task + '_' + DISEASE_LABEL + '_GAP'+ str(GAP.days) + '_MIMIC_IV' + '_' + VERSION + '_case_df.csv'}\"),index_col=0)\n",
    "control_df = pd.read_csv(os.path.join('output',f\"{task + '_' + DISEASE_LABEL + '_GAP'+ str(GAP.days) + '_MIMIC_IV' + '_' + VERSION + '_control_df.csv'}\"),index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating targets and balancing set\n",
    "case_df['y_label'] = np.array([1] * len(case_df))\n",
    "control_df['y_label'] = np.array([0] * len(control_df))\n",
    "\n",
    "print(f\"Positive cases: {len(case_df)}\")\n",
    "print(f\"Negative cases: {len(control_df)}\")\n",
    "\n",
    "control_df = control_df.sample(len(case_df),random_state=42)\n",
    "\n",
    "# Concatenate\n",
    "ml_df = pd.concat([case_df, control_df], axis=0)\n",
    "ml_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Coerce variables to ints\n",
    "ml_df['los_hours'] = ml_df['los_hours'].astype(int)\n",
    "ml_df['age'] = ml_df['age'].astype(int)\n",
    "ml_df['y_label'] = ml_df['y_label'].astype(int)\n",
    "ml_df['comorbs'] = ml_df['comorbs'].astype(int)\n",
    "ml_df['bmi'] = ml_df['bmi'].astype(float)\n",
    "ml_df['bp_systolic'] = ml_df['bp_systolic'].astype(int)\n",
    "ml_df['bp_diastolic'] = ml_df['bp_diastolic'].astype(int)\n",
    "ml_df['presc_meds'] = ml_df['presc_meds'].astype(int)\n",
    "\n",
    "# Create one-hot encodings for categorical features\n",
    "categorical_columns = ['admission_type', 'admission_location',\n",
    "       'insurance', 'ethnicity', 'marital_status', 'gender']  \n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_features = encoder.fit_transform(ml_df[categorical_columns])\n",
    "feature_names = encoder.get_feature_names_out(categorical_columns)\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=feature_names)\n",
    "\n",
    "# Drop original categorical columns and concatenate with encoded ones\n",
    "ml_df = pd.concat([ml_df.drop(categorical_columns, axis=1), encoded_df], axis=1)\n",
    "\n",
    "# Z-score features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Selecting only properly encoded columns\n",
    "ml_df = ml_df[['los_hours',\n",
    "       'age', 'y_label', 'bmi','comorbs','bp_systolic','bp_diastolic', \n",
    "       'presc_meds',\n",
    "       'admission_type_AMBULATORY OBSERVATION', 'admission_type_DIRECT EMER.',\n",
    "       'admission_type_DIRECT OBSERVATION', 'admission_type_ELECTIVE',\n",
    "       'admission_type_EU OBSERVATION', 'admission_type_EW EMER.',\n",
    "       'admission_type_OBSERVATION ADMIT',\n",
    "       'admission_type_SURGICAL SAME DAY ADMISSION', 'admission_type_URGENT',\n",
    "       #'admission_location_AMBULATORY SURGERY TRANSFER',\n",
    "       'admission_location_CLINIC REFERRAL',\n",
    "       'admission_location_EMERGENCY ROOM',\n",
    "       'admission_location_INFORMATION NOT AVAILABLE',\n",
    "       'admission_location_PACU', 'admission_location_PHYSICIAN REFERRAL',\n",
    "       'admission_location_PROCEDURE SITE',\n",
    "       'admission_location_TRANSFER FROM HOSPITAL',\n",
    "       'admission_location_TRANSFER FROM SKILLED NURSING FACILITY',\n",
    "       'admission_location_WALK-IN/SELF REFERRAL', 'insurance_Medicaid',\n",
    "       'insurance_Medicare', 'insurance_Other',\n",
    "       'ethnicity_AMERICAN INDIAN/ALASKA NATIVE', 'ethnicity_ASIAN',\n",
    "       'ethnicity_BLACK/AFRICAN AMERICAN',\n",
    "       'ethnicity_OTHER', \n",
    "       #'ethnicity_UNABLE TO OBTAIN',\n",
    "       'ethnicity_UNKNOWN',\n",
    "       'ethnicity_WHITE', 'marital_status_DIVORCED', 'marital_status_MARRIED',\n",
    "       'marital_status_SINGLE', 'marital_status_WIDOWED', 'marital_status_nan',\n",
    "       'gender_F', 'gender_M']]\n",
    "\n",
    "X = ml_df.drop('y_label', axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = ml_df['y_label']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# GS\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10,],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear','saga'],\n",
    "    'tol': [1e-4, 1e-3, 1e-2], \n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7]\n",
    "    }\n",
    "\n",
    "# Create and fit grid search\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    refit=True,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\") \n",
    "\n",
    "# Training quick model\n",
    "#model = LogisticRegression(penalty='l2', max_iter=5000, C=2)\n",
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.52      0.53       261\n",
      "           1       0.52      0.54      0.53       247\n",
      "\n",
      "    accuracy                           0.53       508\n",
      "   macro avg       0.53      0.53      0.53       508\n",
      "weighted avg       0.53      0.53      0.53       508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "y_pred = grid_search.predict(X_test)\n",
    "#y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                                  estimator_name='LR')\n",
    "display.plot()\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "print(ml_df.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
